{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtQtg62U17dYp9cJiUZOxg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhluu99/minhluu99/blob/main/text_to_img0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw11qZyIp5Kk"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vys9zCFf2jOT",
        "outputId": "bcd2ce42-6cf7-4506-c248-edde92e3e601"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u9uM8B42o-j",
        "outputId": "94052f47-e26a-47a9-f657-65c54c2bae07"
      },
      "source": [
        "%cd '/content/drive/MyDrive/GAN_image'\n",
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/GAN_image\n",
            "\u001b[0m\u001b[01;34mannotations\u001b[0m/  \u001b[01;34mtrain2014\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxagHWF1g4EL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c18d93-e429-4b7d-a5db-a35df073a7f6"
      },
      "source": [
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.3 MB 5.4 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldGH8AIwp4Eu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad949dd-8f52-4d3a-b13a-2f8ba2525fd1"
      },
      "source": [
        "!pip install -q -U tf-models-official"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 26.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 679 kB 46.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 211 kB 49.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 23.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 58.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 48 kB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMnudSaYqCPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2edc7ec-c78d-460e-df2e-4b0a3c77ea8e"
      },
      "source": [
        "!pip install -U tfds-nightly"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tfds-nightly\n",
            "  Downloading tfds_nightly-4.4.0.dev202108040107-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (5.2.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (21.2.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.3.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tfds-nightly) (3.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly) (1.53.0)\n",
            "Installing collected packages: tfds-nightly\n",
            "Successfully installed tfds-nightly-4.4.0.dev202108040107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1UwUHFtrGWt"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlyzcvk0rJRl"
      },
      "source": [
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JyjZn7S2c5G"
      },
      "source": [
        "# **GAN_Setup**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YMXjelO2qej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685916bc-51f3-4d21-be14-42e8071c9b64"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "# example of training an conditional gan on the fashion mnist dataset\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        " \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Concatenate\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w_5xxuQrZXs"
      },
      "source": [
        "# **Connect to the TPU worker**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV2O69_krWwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a553476-a7a3-403e-ba95-c8d4881c9a56"
      },
      "source": [
        "import os\n",
        "\n",
        "if os.environ['COLAB_TPU_ADDR']:\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "  print('Using TPU')\n",
        "elif tf.test.is_gpu_available():\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')\n",
        "else:\n",
        "  raise ValueError('Running on CPU is not recommended.')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mPHRCaMRrlCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b8d027-04c2-4f9c-8917-24bcb59bc75e"
      },
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_uncased_L-24_H-1024_A-16\", \"bert_en_wwm_uncased_L-24_H-1024_A-16\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_en_cased_L-24_H-1024_A-16\", \"bert_en_wwm_cased_L-24_H-1024_A-16\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"albert_en_large\", \"albert_en_xlarge\", \"albert_en_xxlarge\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\", \"talking-heads_large\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'albert_en_large':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_large/2',\n",
        "    'albert_en_xlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_xlarge/2',\n",
        "    'albert_en_xxlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_xxlarge/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "    'talking-heads_large':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_large':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_xlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_xxlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_large':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print('BERT model selected           :', tfhub_handle_encoder)\n",
        "print('Preprocessing model auto-selected:', tfhub_handle_preprocess)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
            "Preprocessing model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1ozHAJlrxv9"
      },
      "source": [
        "# **Preprocess the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElqPbEWorwDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9b19f0-d2d9-4af1-be17-777e94db0640"
      },
      "source": [
        "bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
        "tok = bert_preprocess.tokenize(tf.constant(['Hello TensorFlow!']))\n",
        "print(tok)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[[7592], [23435, 12314], [999]]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq3WX7GAr6QF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "818719c7-245e-45b0-dec0-47dbf9aa74dc"
      },
      "source": [
        "text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], tf.constant(20))\n",
        "\n",
        "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
        "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
        "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
        "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
        "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
        "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Word Ids :  (1, 20)\n",
            "Word Ids       :  tf.Tensor(\n",
            "[  101  7592 23435 12314   999   102  7592 23435 12314   999   102     0\n",
            "     0     0     0     0], shape=(16,), dtype=int32)\n",
            "Shape Mask     :  (1, 20)\n",
            "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0], shape=(16,), dtype=int32)\n",
            "Shape Type Ids :  (1, 20)\n",
            "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0], shape=(16,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-eVjy25s379"
      },
      "source": [
        "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
        "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "  Args:\n",
        "    sentence_features: a list with the names of string-valued features.\n",
        "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\n",
        "    (with the order or names, resp., given by sentence_features) and\n",
        "    returns a dict of tensors for input to BERT.\n",
        "  \"\"\"\n",
        "\n",
        "  input_segments = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "  # Optional: Trim segments in a smart way to fit seq_length.\n",
        "  # Simple cases (like this example) can skip this step and let\n",
        "  # the next step apply a default truncation to approximately equal lengths.\n",
        "  truncated_segments = segments\n",
        "\n",
        "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "  # are model-dependent, so this gets loaded from the SavedModel.\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return tf.keras.Model(input_segments, model_inputs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSEWDW13tD01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef34432-1def-41e7-84be-b5ab81f823cc"
      },
      "source": [
        "test_preprocess_model = make_bert_preprocess_model(['my_input'])\n",
        "test_text = [np.array(['some random test sentence'])]\n",
        "text_preprocessed = test_preprocess_model(test_text)\n",
        "\n",
        "print('Keys           : ', list(text_preprocessed.keys()))\n",
        "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
        "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
        "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
        "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
        "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
        "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys           :  ['input_type_ids', 'input_mask', 'input_word_ids']\n",
            "Shape Word Ids :  (1, 128)\n",
            "Word Ids       :  tf.Tensor(\n",
            "[ 101 2070 6721 3231 6251  102    0    0    0    0    0    0    0    0\n",
            "    0    0], shape=(16,), dtype=int32)\n",
            "Shape Mask     :  (1, 128)\n",
            "Input Mask     :  tf.Tensor([1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n",
            "Shape Type Ids :  (1, 128)\n",
            "Type Ids       :  tf.Tensor([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBMjkUfTu4um",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "457cc9fc-ca75-4ec7-c97e-c209e6d55315"
      },
      "source": [
        "tf.keras.utils.plot_model(test_preprocess_model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsuTuKj3vl2O"
      },
      "source": [
        "# **BERT_model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIOhX-i4INZ2"
      },
      "source": [
        "def build_bert_model():\n",
        "  inputs = dict(\n",
        "      input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "      input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "      input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "  )\n",
        "\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')\n",
        "  net = encoder(inputs)['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(rate=0.1)(net)\n",
        "  return tf.keras.Model(inputs, net, name='prediction')\n",
        "bert_model = build_bert_model()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHcBkZXxDTyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82a93a5-9cf6-4da8-f1be-5d79ab7a9acf"
      },
      "source": [
        "bert_raw_result = bert_model(text_preprocessed)\n",
        "print(bert_raw_result.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot9WueBWzKDn"
      },
      "source": [
        "# **GAN_model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXI-nzk_2-5z"
      },
      "source": [
        "def define_discriminator(in_shape=(64, 64, 3)):\n",
        "\t# label input\n",
        "  in_label = Input(shape=(768,))\n",
        "  n_nodes = in_shape[0] * in_shape[1]\n",
        "  li = Dense(n_nodes)(in_label)\n",
        "  li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "  \n",
        "  #hình ảnh\n",
        "  in_image = Input(shape=in_shape)\n",
        "\t# concat label as a channel\n",
        "  merge = Concatenate()([in_image, li])\n",
        "\t# downsample\n",
        "  fe = Conv2D(64, kernel_size=4, strides=2, padding='same')(merge)\n",
        "  fe = LeakyReLU(alpha=0.2)(fe)\n",
        "  fe = Conv2D(128, kernel_size=4, strides=2, padding='same')(fe)\n",
        "  fe = LeakyReLU(alpha=0.2)(fe)\n",
        "  fe = Conv2D(128, kernel_size=4, strides=2, padding='same')(fe)\n",
        "  fe = LeakyReLU(alpha=0.2)(fe)\n",
        "  fe = Flatten()(fe)\n",
        "  fe = Dropout(0.2)(fe)\n",
        "  out_layer = Dense(1, activation='sigmoid')(fe)\n",
        " \n",
        "\t# define model\n",
        "  model = Model([in_image, in_label], out_layer)\n",
        "\t# compile model\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0TgmwUGDCCR"
      },
      "source": [
        "def define_generator(latent_dim):\n",
        "\t# label input\n",
        "  in_label = Input(shape=(768,))\n",
        "  n_nodes = 8 * 8\n",
        "  li = Dense(n_nodes)(in_label)\n",
        "  li = Reshape((8, 8, 1))(li)\n",
        "  in_lat = Input(shape=(latent_dim,))\n",
        "\n",
        "  n_nodes = 128 * 8 * 8\n",
        "  gen = Dense(n_nodes)(in_lat)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        "  gen = Reshape((8, 8, 128))(gen)\n",
        "\t# merge image gen and label input\n",
        "  merge = Concatenate()([gen, li])\n",
        "\n",
        "  gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        "  gen = Conv2DTranspose(256, (4,4), strides=(2,2), padding='same')(gen)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        "  gen = Conv2DTranspose(512, (4,4), strides=(2,2), padding='same')(gen)\n",
        "  gen = LeakyReLU(alpha=0.2)(gen)\n",
        " \n",
        "\t# output\n",
        "  out_layer = Conv2D(3, (5,5), activation='tanh', padding='same')(gen)\n",
        "  model = Model([in_lat, in_label], out_layer)\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEaqUq24HAGX"
      },
      "source": [
        "def define_gan(g_model, d_model):\n",
        "\td_model.trainable = False\n",
        "\n",
        "\tgen_noise, gen_label = g_model.input\n",
        "\tgen_output = g_model.output\n",
        "\n",
        "\tgan_output = d_model([gen_output, gen_label])\n",
        " \n",
        "\tmodel = Model([gen_noise, gen_input], gan_output)\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPotAqSaEr4a"
      },
      "source": [
        "## **Coco_setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbwkWMDUXinA"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# your model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from PIL import Image"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4YyscY5DvlF",
        "outputId": "21fc5736-939b-408c-c425-c350de12a7f8"
      },
      "source": [
        "%cd"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0-Q1QiIYESg"
      },
      "source": [
        "# Download caption annotation files\n",
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                           extract=True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "# Download image files\n",
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin='http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract=True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFQJ_Hn6como"
      },
      "source": [
        "instances_file = '/content/annotations/instances_train2014.json'\n",
        "annotation_file = '/content/annotations/captions_train2014.json'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKY-4fxOZepU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "414a7dcc-1f44-49f4-f76b-50fbc62d4000"
      },
      "source": [
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "with open(instances_file,'r') as f1:\n",
        "  instances = json.load(f1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-fe30e93e2b38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/annotations/captions_train2014.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApWLiIg6qvw-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "65ed1e8e-e3ff-4bac-ffba-01dda5d45201"
      },
      "source": [
        "imgId=[]\n",
        "for val in instances['categories']:\n",
        "  if val['supercategory'] == 'furniture':\n",
        "    for vall in instances['annotations']:\n",
        "      if vall['category_id']==val['id']:\n",
        "        imgId.append(vall['image_id'])\n",
        "imgId=set(imgId)\n",
        "imgId=list(imgId)\n",
        "print(len(imgId))\n",
        "random.shuffle(imgId)\n",
        "imgId = imgId[:6000]\n",
        "imgId.sort()\n",
        "print(len(imgId))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6266c23b1e54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimgId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categories'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'supercategory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'furniture'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvall\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'instances' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCQgnhIDCj2a"
      },
      "source": [
        "imgId=[]\n",
        "for val in instances['categories']:\n",
        "  if val['name'] == 'bed' or 'couch':\n",
        "    for vall in instances['annotations']:\n",
        "      if vall['category_id']==val['id']:\n",
        "        imgId.append(vall['image_id'])\n",
        "imgId=set(imgId)\n",
        "imgId=list(imgId)\n",
        "print(len(imgId))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq9mt_v8ZfDR"
      },
      "source": [
        "image_path_to_caption = collections.defaultdict(list)\n",
        "for i in imgId:\n",
        "  for val in annotations['annotations']:\n",
        "    if val['image_id'] == i:\n",
        "      caption = [f\" {val['caption']} \"]\n",
        "      image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
        "      image_path_to_caption[image_path].append(caption)\n",
        "      break;\n",
        "print(len(image_path_to_caption))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viya_3pCZj4Q"
      },
      "source": [
        "image_paths = list(image_path_to_caption.keys())\n",
        "random.shuffle(image_paths)\n",
        "\n",
        "# Select the first 6000 image_paths from the shuffled set.\n",
        "# Approximately each image id has 5 captions associated with it, so that will\n",
        "# lead to 30,000 examples.\n",
        "train_image_paths = image_paths[:6000]\n",
        "print(len(train_image_paths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mMJQXcV955c"
      },
      "source": [
        "train_captions = []\n",
        "img_name_vector = []\n",
        "\n",
        "for image_path in train_image_paths:\n",
        "  caption_list = image_path_to_caption[image_path]\n",
        "  train_captions.extend(caption_list)\n",
        "  img_name_vector.extend([image_path] * len(caption_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTwd3B1g-Rr0"
      },
      "source": [
        "print(train_captions[7])\n",
        "Image.open(img_name_vector[7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UBtxMI5CAEZ"
      },
      "source": [
        "def load_real_samples():\n",
        "\t# load dataset\n",
        "\ttrainX = \n",
        "\t# expand to 3d, e.g. add channels\n",
        "\tX = expand_dims(trainX, axis=-1)\n",
        "\t# convert from ints to floats\n",
        "\tX = X.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\treturn [X, trainy]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66hZ-mrVDho8"
      },
      "source": [
        "def load_real_samples(image_paths):\n",
        "  for image_path in image_paths:\n",
        "    X_train = Image.open(image_path)\n",
        "    X_train = np.array(X_train)\n",
        "    X_train = np.resize(X_train,(64,64,3))\n",
        "    X = (X_train - 127.5) / 127.5\n",
        "    caption = \n",
        "  return [X,1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I1bbqLu9yiV"
      },
      "source": [
        "f = Image.open(img_name_vector[7])\n",
        "f = np.array(f)\n",
        "print(type(f))\n",
        "print(f.shape)\n",
        "print(np.resize(f,(64,64,3)).shape)\n",
        "print()\n",
        "x_input = randn(100)\n",
        "print(type(x_input))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}